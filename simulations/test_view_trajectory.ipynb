{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import cvxpy as cp\n",
    "import or_suite\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'Equal_Allocation'\n",
    "problem = 'simple'\n",
    "dir_path = '../data/allocation_%s_%s'%(algorithm,problem)\n",
    "dir_path = dir_path+'/trajectory.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/allocation_Equal_Allocation_simple/trajectory.obj\n"
     ]
    }
   ],
   "source": [
    "print(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_path, 'rb') as f:\n",
    "    x = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 0,\n",
       "  'oldState': array([10.,  2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([9., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 1,\n",
       "  'oldState': array([9., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([8., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 2,\n",
       "  'oldState': array([8., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([7., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 3,\n",
       "  'oldState': array([7., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([6., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 4,\n",
       "  'oldState': array([6., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([5., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 5,\n",
       "  'oldState': array([5., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([4., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 6,\n",
       "  'oldState': array([4., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([3., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 7,\n",
       "  'oldState': array([3., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([2., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 8,\n",
       "  'oldState': array([2., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([1., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 1,\n",
       "  'step': 9,\n",
       "  'oldState': array([1., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([0., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 0,\n",
       "  'oldState': array([10.,  2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([9., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 1,\n",
       "  'oldState': array([9., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([8., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 2,\n",
       "  'oldState': array([8., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([7., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 3,\n",
       "  'oldState': array([7., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([6., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 4,\n",
       "  'oldState': array([6., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([5., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 5,\n",
       "  'oldState': array([5., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([4., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 6,\n",
       "  'oldState': array([4., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([3., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 7,\n",
       "  'oldState': array([3., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([2., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 8,\n",
       "  'oldState': array([2., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([1., 2.]),\n",
       "  'info': {'type': array([2])}},\n",
       " {'iter': 0,\n",
       "  'episode': 2,\n",
       "  'step': 9,\n",
       "  'oldState': array([1., 2.]),\n",
       "  'action': array([[0.5]]),\n",
       "  'reward': array([-0.69314718]),\n",
       "  'newState': array([0., 2.]),\n",
       "  'info': {'type': array([2])}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_envy(X_alg,X_opt,env_config):\n",
    "    \"\"\"\n",
    "    (helper for delta_envy)\n",
    "    Finds maximum envy of X_alg's allocation by comparing its utility to that of X_opt\n",
    "    \"\"\"\n",
    "    u = env_config['utility_function']\n",
    "    w = env_config['weight_matrix']\n",
    "    max_envy=0\n",
    "    for t,allocation in enumerate(X_alg):\n",
    "        for theta,row in enumerate(allocation):\n",
    "            tmp = abs(u(row,w[theta,:])-u(X_opt[t][theta,:],w[theta,:]))\n",
    "            if tmp >= max_envy:\n",
    "                max_envy = tmp\n",
    "    return max_envy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficiency(X_alg, sizes,env_config):\n",
    "    \"\"\"\n",
    "    (helper for delta_efficiency)\n",
    "    Finds efficiency by seeing how much of the initial budget was used in X_alg\n",
    "    \"\"\"\n",
    "    B = env_config['init_budget']\n",
    "    tot_sizes = np.sum(sizes, axis=0)\n",
    "    num_types,num_commodities = env_config['weight_matrix'].shape\n",
    "    return sum([B-sum([tot_sizes[theta]*X_alg[t][theta,:] for theta in range(num_types)]) for t in range(len(X_alg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportionality(X_alg,sizes,env_config):\n",
    "    \"\"\"\n",
    "    (helper for delta_proportionality)\n",
    "    Finds proportionality by calculating envy w.r.t a completely equal allocation\n",
    "    \"\"\"\n",
    "    B = env_config['init_budget']\n",
    "    tot_size = np.sum(sizes)\n",
    "    u = env_config['utility_function']\n",
    "    w = env_config['weight_matrix']\n",
    "    max_prop=0\n",
    "    for t,allocation in enumerate(X_alg):\n",
    "        for theta,row in enumerate(allocation):\n",
    "            tmp = abs(u(row,w[theta,:])-u(B/tot_size,w[theta,:]))\n",
    "            if tmp >= max_prop:\n",
    "                max_prop = tmp\n",
    "    return max_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEANS CODE FOR GENERATING SOLVER FOR OFFLINE PROBLEM\n",
    "def generate_cvxpy_solve(num_types, num_resources):\n",
    "    \"\"\"\n",
    "    Creates a generic solver to solve the offline resource allocation problem\n",
    "    \n",
    "    Inputs: \n",
    "        num_types - number of types\n",
    "        num_resources - number of resources\n",
    "    Returns:\n",
    "        prob - CVXPY problem object\n",
    "        solver - function that solves the problem given data\n",
    "    \"\"\"\n",
    "    x = cp.Variable(shape=(num_types,num_resources))\n",
    "    sizes = cp.Parameter(num_types, nonneg=True)\n",
    "    weights = cp.Parameter((num_types, num_resources), nonneg=True)\n",
    "    budget = cp.Parameter(num_resources, nonneg=True)\n",
    "    objective = cp.Maximize(cp.log(cp.sum(cp.multiply(x, weights), axis=1)) @ sizes)\n",
    "    constraints = []\n",
    "    constraints += [0 <= x]\n",
    "    for i in range(num_resources):\n",
    "        constraints += [x[:, i] @ sizes <= budget[i]]\n",
    "    # constraints += [x @ sizes <= budget]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    def solver(true_sizes, true_weights, true_budget):\n",
    "        sizes.value = true_sizes\n",
    "        weights.value = true_weights\n",
    "        budget.value = true_budget\n",
    "        prob.solve()\n",
    "        return prob.value, np.around(x.value, 5)\n",
    "    return prob, solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_opt(budget, size, weights, solver):\n",
    "    \"\"\"\n",
    "    Uses solver from generate_cvxpy_solve and applies it to values\n",
    "    \n",
    "    Inputs:\n",
    "        budget: initial budget for K commodities\n",
    "        size: 2D numpy array of sizes of each type at each location\n",
    "        weights: 2D numpy array containing the demands of each type\n",
    "    \"\"\"\n",
    "    tot_size = np.sum(size, axis=0)\n",
    "    _, x = solver(tot_size, weights, budget)\n",
    "    allocation = np.zeros((size.shape[0], weights.shape[0], weights.shape[1]))\n",
    "    for i in range(size.shape[0]):\n",
    "        allocation[i,:,:] = x\n",
    "    return allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_envy(traj, env_config):\n",
    "    \"\"\"\n",
    "    Calculates the delta_envy metric given the trajectory of a given algorithm\n",
    "    \n",
    "    Inputs:\n",
    "        traj: trajectory of an algorithm, stored as a list of dictionaries\n",
    "        env_config: configuration of the environment\n",
    "    Returns:\n",
    "        final_avg_envies: array of the average envy for each episode\n",
    "        \n",
    "    WHY ARE EPISODES INDEXED BY 1 PLEASE FIX\n",
    "    \"\"\"\n",
    "    import cvxpy as cp\n",
    "    num_iter = traj[-1]['iter']+1\n",
    "    num_eps = traj[-1]['episode']+1\n",
    "    num_steps = traj[-1]['step']+1\n",
    "    #print('Iters: %s, Eps: %s, Steps: %s'%(num_iter,num_eps,num_steps))\n",
    "    num_types,num_commodities = traj[-1]['action'].shape \n",
    "    final_avg_envies = np.zeros(num_eps-1)\n",
    "    \n",
    "    for iteration in range(num_iter):      \n",
    "        iteration_traj = list(filter(lambda d: d['iter']==iteration, traj))\n",
    "        \n",
    "        for ep in range(1,num_eps):\n",
    "            ep_traj = list(filter(lambda d: d['episode']==ep, traj))\n",
    "            sizes = np.zeros((num_steps,num_types))\n",
    "\n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                size = step_dict['info']['type']\n",
    "                sizes[idx,:] = size\n",
    "                   \n",
    "            prob, solver = generate_cvxpy_solve(num_types,num_commodities)\n",
    "            X_opt = offline_opt(env_config['init_budget'],sizes,env_config['weight_matrix'],solver)\n",
    "            X_alg = np.zeros((num_steps,num_types,num_commodities))\n",
    "            \n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                X_alg[idx,:,:] = step_dict['action']\n",
    "            \n",
    "            envy = get_envy(X_alg,X_opt,env_config)\n",
    "            final_avg_envies[ep-1] += (1/num_iter)*envy\n",
    "            #print(\"Envy for episode %s: %s\"%(ep,envy))\n",
    "            \n",
    "    return np.mean(final_avg_envies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_efficiency(traj, env_config):\n",
    "    \"\"\"\n",
    "    Calculate the efficiency (waste) of an algorithm given its trajectory\n",
    "    \n",
    "    Inputs:\n",
    "        traj: trajectory of an algorithm, stored as a list of dictionaries\n",
    "        env_config: configuration of the environment\n",
    "    Returns:\n",
    "        final_avg_efficiency: array containing average waste per episode \n",
    "\n",
    "    WHY ARE EPISODES 1 INDEXED BUT ITERATIONS AND STEPS NOT PLEASE FIX\n",
    "    \"\"\"\n",
    "    import cvxpy as cp\n",
    "    num_iter = traj[-1]['iter']+1\n",
    "    num_eps = traj[-1]['episode']+1\n",
    "    num_steps = traj[-1]['step']+1\n",
    "    #print('Iters: %s, Eps: %s, Steps: %s'%(num_iter,num_eps,num_steps))\n",
    "    num_types,num_commodities = traj[-1]['action'].shape \n",
    "    final_avg_efficiency = np.zeros(num_eps-1)\n",
    "    for iteration in range(num_iter):      \n",
    "        iteration_traj = list(filter(lambda d: d['iter']==iteration, traj))\n",
    "\n",
    "        for ep in range(1,num_eps):\n",
    "            ep_traj = list(filter(lambda d: d['episode']==ep, traj))\n",
    "            sizes = np.zeros((num_steps,num_types))\n",
    "\n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                size = step_dict['info']['type']\n",
    "                sizes[idx,:] = size\n",
    "            \n",
    "            X_alg = np.zeros((num_steps,num_types,num_commodities))\n",
    "            \n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                X_alg[idx,:,:] = step_dict['action']\n",
    "            \n",
    "            eff = get_efficiency(X_alg,sizes,env_config)\n",
    "            final_avg_efficiency[ep-1] += (1/num_iter)*eff\n",
    "            #print(\"Efficiency for episode %s: %s\"%(ep,eff))\n",
    "\n",
    "    return np.mean(final_avg_efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_proportionality(traj, env_config):\n",
    "    \"\"\"\n",
    "    Calculate the proportionality (distance to equal allocation) at each episode\n",
    "    \n",
    "    Inputs:\n",
    "        traj: trajectory of an algorithm, stored as a list of dictionaries\n",
    "        env_config: configuration of the environment\n",
    "    Returns:\n",
    "        final_avg_efficiency: array containing average waste per episode \n",
    "    \n",
    "    WHY ARE EPISODES 1 INDEXED BUT ITERATIONS AND STEPS NOT PLEASE FIX\n",
    "    \"\"\"\n",
    "    import cvxpy as cp\n",
    "    \n",
    "    num_iter = traj[-1]['iter']+1\n",
    "    num_eps = traj[-1]['episode']+1\n",
    "    num_steps = traj[-1]['step']+1\n",
    "    #print('Iters: %s, Eps: %s, Steps: %s'%(num_iter,num_eps,num_steps))\n",
    "    num_types,num_commodities = traj[-1]['action'].shape \n",
    "    final_avg_efficiency = np.zeros(num_eps-1)\n",
    "    \n",
    "    for iteration in range(num_iter):      \n",
    "        iteration_traj = list(filter(lambda d: d['iter']==iteration, traj))\n",
    "\n",
    "        for ep in range(1,num_eps):\n",
    "\n",
    "            ep_traj = list(filter(lambda d: d['episode']==ep, traj))\n",
    "            sizes = np.zeros((num_steps,num_types))\n",
    "\n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                size = step_dict['info']['type']\n",
    "                sizes[idx,:] = size\n",
    "\n",
    "            X_alg = np.zeros((num_steps,num_types,num_commodities))\n",
    "            \n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                X_alg[idx,:,:] = step_dict['action']\n",
    "            \n",
    "            prop = get_proportionality(X_alg,sizes,env_config)\n",
    "            final_avg_efficiency[ep-1] += (1/num_iter)*prop\n",
    "            #print(\"Proportionality for episode %s: %s\"%(ep,prop))\n",
    "            \n",
    "    return np.mean(final_avg_efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_OPT(traj, env_config):\n",
    "    \"\"\"\n",
    "    Calculates the distance to X_opt w.r.t supremum norm\n",
    "    \n",
    "    Inputs:\n",
    "        traj: trajectory of an algorithm, stored as a list of dictionaries\n",
    "        env_config: configuration of the environment\n",
    "    Returns:\n",
    "        final_avg_dist: array of the average dist to X_opt for each episode\n",
    "        \n",
    "    WHY ARE EPISODES INDEXED BY 1 PLEASE FIX\n",
    "    \"\"\"\n",
    "    import cvxpy as cp\n",
    "    num_iter = traj[-1]['iter']+1\n",
    "    num_eps = traj[-1]['episode']+1\n",
    "    num_steps = traj[-1]['step']+1\n",
    "    #print('Iters: %s, Eps: %s, Steps: %s'%(num_iter,num_eps,num_steps))\n",
    "    num_types,num_commodities = traj[-1]['action'].shape \n",
    "    final_avg_dist = np.zeros(num_eps-1)\n",
    "    \n",
    "    for iteration in range(num_iter):      \n",
    "        iteration_traj = list(filter(lambda d: d['iter']==iteration, traj))\n",
    "        \n",
    "        for ep in range(1,num_eps):\n",
    "            ep_traj = list(filter(lambda d: d['episode']==ep, traj))\n",
    "            sizes = np.zeros((num_steps,num_types))\n",
    "\n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                size = step_dict['info']['type']\n",
    "                sizes[idx,:] = size\n",
    "                   \n",
    "            prob, solver = generate_cvxpy_solve(num_types,num_commodities)\n",
    "            X_opt = offline_opt(env_config['init_budget'],sizes,env_config['weight_matrix'],solver)\n",
    "            X_alg = np.zeros((num_steps,num_types,num_commodities))\n",
    "            \n",
    "            for idx,step_dict in enumerate(ep_traj):\n",
    "                X_alg[idx,:,:] = step_dict['action']\n",
    "            \n",
    "            dist = np.max(np.absolute(X_opt-X_alg))\n",
    "            final_avg_dist[ep-1] += (1/num_iter)*dist\n",
    "            #print(\"Dist to OPT for episode %s: %s\"%(ep,dist))\n",
    "            \n",
    "    return np.mean(final_avg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    }
   ],
   "source": [
    "ENV_CONFIG = {'K':1,\n",
    "  'num_rounds':10,\n",
    "  'weight_matrix': np.array([[1]]),\n",
    "  'init_budget': np.array([10.]),\n",
    "  'utility_function': lambda x,theta: x,\n",
    "  'type_dist': lambda i : np.array([2])\n",
    "}\n",
    "envy = delta_envy(x,env_config=ENV_CONFIG)\n",
    "efficiency = delta_efficiency(x, env_config=ENV_CONFIG)\n",
    "proportionality = delta_proportionality(x, env_config=ENV_CONFIG)\n",
    "dist_to_OPT = delta_OPT(x,env_config=ENV_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envy: 0.0\n",
      "Efficiency: 0.0\n",
      "Proportionality: 0.0\n",
      "Distance to OPT: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Envy: %s\"%envy)\n",
    "print(\"Efficiency: %s\"%efficiency)\n",
    "print(\"Proportionality: %s\"%proportionality)\n",
    "print(\"Distance to OPT: %s\"%dist_to_OPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
